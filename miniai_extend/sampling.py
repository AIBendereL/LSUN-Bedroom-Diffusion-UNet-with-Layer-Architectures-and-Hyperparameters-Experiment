# AUTOGENERATED! DO NOT EDIT! File to edit: ../image_generator_norm_layernorm_layernorm_sampling.ipynb.

# %% auto 0
__all__ = ['device', 'sampling', 'latents_to_images']

# %% ../image_generator_norm_layernorm_layernorm_sampling.ipynb 110
import torch

import fastcore.all as fc
from fastprogress import progress_bar

# %% ../image_generator_norm_layernorm_layernorm_sampling.ipynb 111
from miniai.learner import to_cpu

# %% ../image_generator_norm_layernorm_layernorm_sampling.ipynb 112
device = "cuda" if torch.cuda.is_available() else "cpu"
device

# %% ../image_generator_norm_layernorm_layernorm_sampling.ipynb 113
def sampling(sched, model, steps, size, eta= fc.noop, device= device):

    model.eval()
    samples = []

    sched.set_timesteps(steps)
    xt = torch.randn(size).to(device)

    for t in progress_bar(sched.timesteps):

        with torch.no_grad(): noise = model(xt, t).sample
        xt = sched.step(noise, t, xt, eta= eta).prev_sample
        
        samples.append(to_cpu(xt))

    return samples

# %% ../image_generator_norm_layernorm_layernorm_sampling.ipynb 114
def latents_to_images(vae, latents, clamp= True, device= device):

    imgs = vae.decode(latents.to(device)).sample
    if clamp: imgs = imgs.clamp(0, 1)
    return to_cpu(imgs)
